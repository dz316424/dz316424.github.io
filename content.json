{"meta":{"title":"Distributed Knowledge","subtitle":null,"description":null,"author":"Haoliang Yu","url":"http://haoliangyu.github.io/blog"},"pages":[{"title":"all-archives","date":"2018-03-03T20:45:36.138Z","updated":"2018-03-03T20:45:36.138Z","comments":true,"path":"all-archives/index.html","permalink":"http://haoliangyu.github.io/blog/all-archives/index.html","excerpt":"","text":""},{"title":"all-categories","date":"2018-03-03T20:45:36.138Z","updated":"2018-03-03T20:45:36.138Z","comments":true,"path":"all-categories/index.html","permalink":"http://haoliangyu.github.io/blog/all-categories/index.html","excerpt":"","text":""},{"title":"all-tags","date":"2018-03-03T20:45:36.138Z","updated":"2018-03-03T20:45:36.138Z","comments":true,"path":"all-tags/index.html","permalink":"http://haoliangyu.github.io/blog/all-tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Using RxJS with PostgreSQL","slug":"Using-RxJS-with-PostgreSQL","date":"2017-04-13T18:47:46.000Z","updated":"2018-03-03T20:45:36.122Z","comments":true,"path":"2017/04/13/Using-RxJS-with-PostgreSQL/","link":"","permalink":"http://haoliangyu.github.io/blog/2017/04/13/Using-RxJS-with-PostgreSQL/","excerpt":"Reactive programming is an interesting way in JavaScript to handle asynchronous actions. Some aspects of its nature makes it a powerful tool to build Node.js application that handles large volume of data in the database. In this article, I am going to introduce RxJS, the most widely used reactive programming library, and how to use it handle the PostgreSQL database operations.","text":"Reactive programming is an interesting way in JavaScript to handle asynchronous actions. Some aspects of its nature makes it a powerful tool to build Node.js application that handles large volume of data in the database. In this article, I am going to introduce RxJS, the most widely used reactive programming library, and how to use it handle the PostgreSQL database operations.What’s reactive programming?","categories":[],"tags":[]},{"title":"The unofficial documentation for ArcGIS Open Data APIs","slug":"The-unofficial-documentation-for-ArcGIS-Open-Data-dataset-search-API","date":"2017-02-11T16:48:55.000Z","updated":"2018-03-03T20:45:36.122Z","comments":true,"path":"2017/02/11/The-unofficial-documentation-for-ArcGIS-Open-Data-dataset-search-API/","link":"","permalink":"http://haoliangyu.github.io/blog/2017/02/11/The-unofficial-documentation-for-ArcGIS-Open-Data-dataset-search-API/","excerpt":"ArcGIS Open Data is a great network of governmental and institutional open data. While the data portal service runs on APIs, there is no official documentation of public APIs for ArcGIS Open Data. So this post is to provide the unofficial documentation of ArcGIS Open Data APIs, which are discovered in my work (mostly by accident).","text":"ArcGIS Open Data is a great network of governmental and institutional open data. While the data portal service runs on APIs, there is no official documentation of public APIs for ArcGIS Open Data. So this post is to provide the unofficial documentation of ArcGIS Open Data APIs, which are discovered in my work (mostly by accident).This documentation is also published at GitHub Gist;Dataset Search APIExample1http://data.portal.com/datasets?q=testParametersq (string)query string for full text searchbbox (unknown)boundary box for geographic searchrequired_keywords (unknown)dataset keywords (tags) for searchpage (integer)current page of resultsper_page (integer)number of results per page (default: 10)sort_by (string)returned results sorting method:updated_at (default)relevancesort_order (string)returned results order:desc (default)ascResponse Example123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126&#123; \"data\": [ &#123; \"display_field\": \"FAC_CAT\", \"max_record_count\": 2000, \"record_count\": 11, \"geometry_type\": \"esriGeometryPoint\", \"object_id_field\": \"FID\", \"supported_extensions\": \"\", \"advanced_query_capabilities\": &#123; \"supports_pagination\": true, \"supports_query_related_pagination\": true, \"supports_query_with_distance\": true, \"supports_returning_query_extent\": true, \"supports_statistics\": true, \"supports_order_by\": true, \"supports_distinct\": true, \"supports_query_with_result_type\": true, \"supports_sql_expression\": true, \"supports_advanced_query_related\": true, \"supports_returning_geometry_centroid\": false &#125;, \"supports_advanced_queries\": true, \"id\": \"dd62922ee5c14d11a187aaf30052404f_0\", \"landing_page\": \"https://www.arcgis.com/home/item.html?id=dd62922ee5c14d11a187aaf30052404f\", \"description\": \"This feature layer, utilizing data from the U.S. Environmental Protection Agency (EPA), displays regional offices. The EPA began operation on December 2, 1970 and it inherited two regional systems from predecessor agencies. The Federal Water Quality Administration which used a nine region system and the Environmental Health Service which had adopted the ten Standard Federal Regions suggested by the Office of Management and Budget (OMB). In order to facilitate easier operations with local and state governments as well as other federal agencies the EPA chose to adopt the OMB Standard Federal Regions which still exist today.&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;div&gt;&lt;img src=\\\"http://fedmaps.maps.arcgis.com/sharing/rest/content/items/26b532f4bfb14e0eb517c644dcda73c1/data\\\" /&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;i&gt;Regional Office locations&lt;/i&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;For more information: &lt;a href=\\\"https://www.epa.gov/aboutepa\\\" target=\\\"_blank\\\"&gt;About EPA&lt;/a&gt;&lt;/div&gt;&lt;div&gt;For feedback, please contact: &lt;a href=\\\"mailto:ArcGIScomNationalMaps@esri.com\\\" target=\\\"_blank\\\"&gt;ArcGIScomNationalMaps@esri.com&lt;/a&gt;&lt;/div&gt;&lt;div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;div&gt;EPA sites of interest&lt;b&gt;&lt;br /&gt;&lt;/b&gt;&lt;/div&gt;&lt;div&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;&lt;/div&gt;&lt;div&gt;&lt;img src=\\\"http://fedmaps.maps.arcgis.com/sharing/rest/content/items/d543cdf1e9694f5f985eae5b2dcd36f8/data\\\" /&gt; &lt;a href=\\\"https://epa.maps.arcgis.com/home/index.html\\\" target=\\\"_blank\\\"&gt;ArcGIS Online Organizational Homepage&lt;/a&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;&lt;/div&gt;&lt;div&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;&lt;/div&gt;&lt;div&gt;Other Federal User Community federally focused content that may interest you&lt;/div&gt;&lt;div&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;&lt;/div&gt;&lt;div&gt;&lt;img src=\\\"http://fedmaps.maps.arcgis.com/sharing/rest/content/items/f83c3452ec074ee08c7f04975578c212/data\\\" /&gt; &lt;a href=\\\"http://fedmaps.maps.arcgis.com/home/search.html?q=owner%3AFederal_User_Community%20AND%20tags%3AUS%20EPA&amp;amp;t=content&amp;amp;restrict=false\\\" target=\\\"_blank\\\"&gt;U.S. Environmental Protection Agency&lt;/a&gt; &lt;img src=\\\"http://fedmaps.maps.arcgis.com/sharing/rest/content/items/46785cfb399344a6af50d4514d6ef0f9/data\\\" /&gt; &lt;a href=\\\"http://open.fedmaps.opendata.arcgis.com/datasets?q=US+EPA&amp;amp;sort_by=relevance\\\" target=\\\"_blank\\\"&gt;Open Data: U.S. EPA&lt;/a&gt; &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;\", \"extent\": &#123; \"coordinates\": [ [ -125.676, 24.242 ], [ -65.559, 50.089 ] ] &#125;, \"fields\": [ &#123; \"name\": \"FID\", \"type\": \"esriFieldTypeInteger\", \"alias\": \"FID\", \"domain\": null, \"statistics\": &#123; \"duration\": 0 &#125;, \"updated_at\": \"2017-02-06T21:06:49.399Z\" &#125; ], \"item_name\": \"EPA Regional Offices\", \"type\": \"ItemLayer\", \"item_type\": \"Feature Layer\", \"license\": \"&lt;p&gt;&lt;img src=\\\"http://downloads.esri.com/blogs/arcgisonline/esrilogo_new.png\\\" /&gt;This work is licensed under the Esri Master License Agreement.&lt;br /&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=\\\"http://links.esri.com/tou_summary\\\" target=\\\"_blank\\\"&gt;View Summary&lt;/a&gt; | &lt;a href=\\\"http://links.esri.com/agol_tou\\\" target=\\\"_blank\\\"&gt;View Terms of Use&lt;/a&gt;&lt;/p&gt;\", \"name\": \"EPA Regional Offices\", \"owner\": \"Federal_User_Community\", \"tags\": [ \"A-16\", \"A16\", \"U.S. Environmental Protection Agency\", \"U.S. EPA\", \"EPA\", \"regional offices\", \"regions\", \"Environmental Protection Agency\", \"places\", \"boundaries\" ], \"thumbnail_url\": \"https://www.arcgis.com/sharing/rest/content/items/dd62922ee5c14d11a187aaf30052404f/info/thumbnail/EPA_-_Regional_Offices_-_screen_capture.png\", \"public\": true, \"created_at\": \"2016-09-02T11:50:55.000Z\", \"updated_at\": \"2017-02-06T21:06:50.911Z\", \"url\": \"https://services2.arcgis.com/FiaPA4ga0iQKduv3/arcgis/rest/services/EPA_RegionalOffices/FeatureServer/0\", \"views\": null, \"quality\": 86, \"coverage\": \"global\", \"current_version\": 10.41, \"comments_enabled\": true, \"service_spatial_reference\": &#123; \"wkid\": 102100, \"latestWkid\": 3857 &#125;, \"metadata_url\": null, \"org_id\": \"FiaPA4ga0iQKduv3\", \"metadata\": &#123; \"published\": null, \"present\": false, \"url\": null, \"online_resources\": [] &#125;, \"structured_license\": &#123; \"type\": \"custom\", \"text\": \"This work is licensed under the Esri Master License Agreement.&lt;a href=\\\"http://links.esri.com/tou_summary\\\" target=\\\"_blank\\\"&gt;View Summary&lt;/a&gt; | &lt;a href=\\\"http://links.esri.com/agol_tou\\\" target=\\\"_blank\\\"&gt;View Terms of Use&lt;/a&gt;\" &#125;, \"use_standardized_queries\": true, \"sites\": [ &#123; \"title\": \"PACI\", \"url\": \"http://paci-esridubaioffice.opendata.arcgis.com\", \"logo\": null &#125; ], \"main_group_title\": \"Open Data - Derived US Independent Establishments and Gov't Corps\", \"main_group_description\": \"&lt;span style='line-height: 24px; background-color: rgb(255, 255, 255);'&gt;&lt;font face='Verdana' size='3'&gt;The group contains a set of map services, web maps and map packages that can be used in a web browser or downloaded to your ArcGIS Desktop application. The maps may be used as base maps and operational layers to support a variety of applications.&lt;/font&gt;&lt;/span&gt;\", \"main_group_thumbnail_url\": null &#125; ], \"metadata\": &#123; \"query_parameters\": &#123; \"bbox\": null, \"page\": 1, \"per_page\": 1, \"q\": \"*\", \"required_keywords\": [], \"sort_by\": \"updated_at\", \"sort_order\": \"desc\" &#125;, \"stats\": &#123; \"count\": 1, \"total_count\": 192, \"top_tags\": [ &#123; \"name\": \"ocean\", \"count\": 62 &#125; ] &#125; &#125;&#125;","categories":[{"name":"Open Data","slug":"Open-Data","permalink":"http://haoliangyu.github.io/blog/categories/Open-Data/"}],"tags":[{"name":"arcgis","slug":"arcgis","permalink":"http://haoliangyu.github.io/blog/tags/arcgis/"},{"name":"open data","slug":"open-data","permalink":"http://haoliangyu.github.io/blog/tags/open-data/"}]},{"title":"Boundary.Now: a simple tool to extract OSM place boundaries","slug":"Boundary-Now-a-simple-tool-to-extract-OSM-place-boundaries","date":"2017-02-04T22:15:56.000Z","updated":"2017-02-26T05:00:00.000Z","comments":true,"path":"2017/02/04/Boundary-Now-a-simple-tool-to-extract-OSM-place-boundaries/","link":"","permalink":"http://haoliangyu.github.io/blog/2017/02/04/Boundary-Now-a-simple-tool-to-extract-OSM-place-boundaries/","excerpt":"Getting a place boundary is sometimes harder than it looks like.","text":"Getting a place boundary is sometimes harder than it looks like.There are a couple of reasons why it’s hard:The boundary data is actually unavailable or not released.The boundary data is available, but packaged in a national dataset, like most governmental open data.The boundary data is visible, but not downloadable, like the Google map does.The boundary data is exportable, but the use is limited, like the Bing map geocoding API does.So it’s pretty common for a data analyst to take a lot of effort to find and extract place boundaries. It’s even more painful when we have to download national datasets just for a few cities’ boundary.Oh, wait! Isn’t it a free and global geospatial database out there?Yeah, OpenStreetMap is awesome and the great Nominatim service allows us to search through the OSM database with a place name.The only problem is the Nominatim interface is not designed for data extraction, even though the API does return place boundary. So I develop Boundary.Now, an interface for Nominatim geocoding API to download place boundary.Like the original interface, it will provide a search box for the desired place name. In the result list, only search results with boundary data are shown. The boundary data could be downloaded in GeoJSON, a format widely used in web development, or in Shapefile, a standard format in the GIS world.You can open the tool here and the project is open at GitHub.","categories":[{"name":"Project","slug":"Project","permalink":"http://haoliangyu.github.io/blog/categories/Project/"}],"tags":[{"name":"gis","slug":"gis","permalink":"http://haoliangyu.github.io/blog/tags/gis/"},{"name":"openstreetmap","slug":"openstreetmap","permalink":"http://haoliangyu.github.io/blog/tags/openstreetmap/"}]},{"title":"Adding GeoJSON layers to your Leaflet map in TypeScript","slug":"Adding-GeoJSON-layers-in-your-Leaflet-project-in-TypeScript","date":"2017-02-04T05:53:35.000Z","updated":"2018-03-03T20:45:36.102Z","comments":true,"path":"2017/02/04/Adding-GeoJSON-layers-in-your-Leaflet-project-in-TypeScript/","link":"","permalink":"http://haoliangyu.github.io/blog/2017/02/04/Adding-GeoJSON-layers-in-your-Leaflet-project-in-TypeScript/","excerpt":"Adding a GeoJSON layer to a Leaflet map in TypeScript has some difference from that in JavaScript.","text":"Adding a GeoJSON layer to a Leaflet map in TypeScript has some difference from that in JavaScript.It’s because the GeoJSON is typed in TypeScript and the type declaration of Leaflet adopts it. So whenever you construct the GeoJSON object in TypeScript, it’s important to declare the type of the variable as GeoJSON type:1234567891011121314151617// it's necessary to tell variable typelet featureCollection: GeoJSON.FeatureCollection&lt;any&gt; = &#123; type: 'FeatureCollection', features: [ &#123; type: 'Feature', geometry: &#123; type: 'Point', coordinates: [0, 0] &#125;, properties: &#123;&#125; &#125; ]&#125;;// the Leaflet API is the same as the JavaScript one, except for the parameter type requirementL.geoJSON(featureCollection).addTo(this.map);For more about the specification and example, take a look at the GeoJSON type definition.","categories":[{"name":"GIS","slug":"GIS","permalink":"http://haoliangyu.github.io/blog/categories/GIS/"}],"tags":[{"name":"geojson","slug":"geojson","permalink":"http://haoliangyu.github.io/blog/tags/geojson/"},{"name":"leaflet","slug":"leaflet","permalink":"http://haoliangyu.github.io/blog/tags/leaflet/"},{"name":"typescript","slug":"typescript","permalink":"http://haoliangyu.github.io/blog/tags/typescript/"}]},{"title":"Using an untyped Leaflet plugin in your TypeScript project","slug":"Using-an-untyped-Leaflet-plugin-in-your-TypeSccript-project","date":"2017-01-25T05:00:00.000Z","updated":"2018-03-03T20:45:36.122Z","comments":true,"path":"2017/01/25/Using-an-untyped-Leaflet-plugin-in-your-TypeSccript-project/","link":"","permalink":"http://haoliangyu.github.io/blog/2017/01/25/Using-an-untyped-Leaflet-plugin-in-your-TypeSccript-project/","excerpt":"Leaflet has a prosperous ecosystem with hundreds of plugins. Most of them are not typed for TypeScript, however, with a minimal setup, you are able to use these plugins in your TypeScript mapping project.","text":"Leaflet has a prosperous ecosystem with hundreds of plugins. Most of them are not typed for TypeScript, however, with a minimal setup, you are able to use these plugins in your TypeScript mapping project.Since TypeScript is definitely typed, a simple workaround is to provide a minimal type declaration file that can expose the plugin functions in the L namespace.For example, I would like to display a large GeoJSON file at the browser using the vector tile technique. I can do it in JavaScript with the following code:12345678// Get the GeoJSON somewherelet geojson = getGeoJSON();// Slice the GeoJSON into vector tiles on-the-fly with L.vectorGrid.slicer()let layer = L.vectorGrid.slicer(geojson, &#123;&#125;);// Add the tile layer on the maplayer.addTo(map);The vector tile plugin Leaflet.VectorGrid isn’t officially typed and you are not able to directly use this plugin because the compiler doesn’t know its existence. So we just need to declare it with a leaflet.vectorgrid.d.ts file123456789101112// in the global namesapce \"L\"declare namespace L &#123; // there is a child namespace \"vectorGrid\" namespace vectorGrid &#123; // which has a function call \"slicer\" that takes data and optional // configurations. To make it simple, we don't specify the input // and output types. export function slicer(data: any, options?: any): any; &#125;&#125;Then we are able to use the function in TypeScript123456789/// &lt;reference path=\"leaflet.vectorgrid.d.ts\"/&gt;import 'leaflet';import 'leaflet.vectorgrid';let geojson = getGeoJSON();let layer = L.vectorGrid.slicer(geojson)layer.addTo(map);Here we go!If we want to use more, we could continue to populate the type declaration file and maybe contribute it to the community when it becomes more complete.For a fully functional exampe, see angular2-leaflet-starter.","categories":[{"name":"GIS","slug":"GIS","permalink":"http://haoliangyu.github.io/blog/categories/GIS/"}],"tags":[{"name":"leaflet","slug":"leaflet","permalink":"http://haoliangyu.github.io/blog/tags/leaflet/"},{"name":"typescript","slug":"typescript","permalink":"http://haoliangyu.github.io/blog/tags/typescript/"}]},{"title":"Celebrating the 1,000 data portals at OpenDataDiscovery.org","slug":"Celebrating-the-1-000-data-portals-at-OpenDataDiscovery-org","date":"2017-01-16T05:00:00.000Z","updated":"2018-03-03T20:45:36.110Z","comments":true,"path":"2017/01/16/Celebrating-the-1-000-data-portals-at-OpenDataDiscovery-org/","link":"","permalink":"http://haoliangyu.github.io/blog/2017/01/16/Celebrating-the-1-000-data-portals-at-OpenDataDiscovery-org/","excerpt":"I am pleased to announce that OpenDataDiscovery.org is now tracking more than 1,000 open data portals, 40% of data portals in the world!","text":"I am pleased to announce that OpenDataDiscovery.org is now tracking more than 1,000 open data portals, 40% of data portals in the world!A map of all recorded 329 cities / provinces / countries worldwide, in which each color category includes 20% of all portals.Thanks to OpenDataSoft‘s comprehensive open data portal survey (see their blog), I am able to quickly identify a list of important open data service providers. So far seven major open data service providers have been supported:ArcGIS Open Data (595 portals, 26,281 datasets)CKAN (89 portals, 842,154 datasets)DKAN (40 portals, 14,289 datasets)GeoNode (10 portals, 9,604 datasets)Junar (11 portals, 1,860 datasets)OpenDataSoft (79 portals, 6,084 datasets)Socrata (177 portals, 110,130 datasets)As of 01/16/2017, the total number of datasets opened at these portals is more than 1 million! Also, thousands of data categories and publishers, and nearly a million data tags have been identified. This project just reveals an immense and existing world of open data and all these numbers are still likely to be underestimated.For the following months, I will focus on analyzing the existing/coming data and building better data visualization. As the portal level metadata collection procedure is close to done, it is also time to explore the collection of the dataset level metadata.I can wait to work on it to see what story this data is telling!","categories":[{"name":"GIS","slug":"GIS","permalink":"http://haoliangyu.github.io/blog/categories/GIS/"}],"tags":[{"name":"gis","slug":"gis","permalink":"http://haoliangyu.github.io/blog/tags/gis/"},{"name":"OpenDataDiscovery.org","slug":"OpenDataDiscovery-org","permalink":"http://haoliangyu.github.io/blog/tags/OpenDataDiscovery-org/"},{"name":"open data","slug":"open-data","permalink":"http://haoliangyu.github.io/blog/tags/open-data/"}]},{"title":"Work, Life, 2016","slug":"Work-Life-2016","date":"2016-12-31T05:00:00.000Z","updated":"2018-03-03T20:45:36.122Z","comments":true,"path":"2016/12/31/Work-Life-2016/","link":"","permalink":"http://haoliangyu.github.io/blog/2016/12/31/Work-Life-2016/","excerpt":"2016 is a big year for me.","text":"2016 is a big year for me.This is the first year after my graduation. I moved away from the university and settled down at downtown to start my new life. Being a web developer in a small startup is very different from being a student in school. In school there are always paths to follow: the spring and fall curriculum, the study plan of the course, the mentorship of the professor. In a startup, things are less organized and more dynamic. I make my work plan, develop at both ends of our application, help GIS Analysts in the company to automate data pipeline, discuss new features with clients, lead internal code safari, and experiment new technologies for our platform. Every day is a new and challenging day. That’s very exciting.Of course challenges mean a lot of work and the trial to keep up with this fast-peace industry means even more. I become a full-time developer. I mean full-time as day and night.My colleague said I was having two jobs: one at the daytime working in the office, and another at the nighttime working at home. That’s true, though the nighttime job is not billable and sometimes more challenging than the daytime one.One cruel fact of this industry is that the concepts, trends, and technologies can be changed in just 2 or 3 years. It’s harder when you are not in a CS major (yeah I am the old-class geographer!). At 2016 I took numerous hours to stay with the leading wave of techs:Learn how to write inJavaScript ES6Learn how to develop application with Angular 2 and TypeScriptPlay with Bootstrap and Material DesignUnderstand vector tile and figure out the way to generate it and use it with Leaflet / Mapbox GLGet deeper with DevOps tools like Vagrant, Docker,and NGNIXGet familar with AWS EC2 and AutoScale serviceI also spent much time on data structures and algorithms in order to be a qualified software engineer in the future.But on the other hand, the software industry is warm as we believe in open source and knowledge sharing. GitHub, StackOverflow, The Spatial Community Slack Channel, and many other tech blogs have been my primary learning sources. I am glad that I was introduced into the rapidly developing world of open source GIS. Their generous sharing has been so beneficial to my work and side-projects.With the hope of helping the future newbee, I started to publish open source projects with the highest quality I could pursue. This is my nighttime job :-P. So far, I have done9 npm modules (see here)2 Angular 2 mapping project demo with Leaflet or Mapbox GL1 fully operating website (OpenDataDiscovery.org)29 pull requests to community projectsI have got the opportunity to learn and test new knowledge that I don’t use in my daytime job. As a benefit, I can move ahead the company and be able to introduce what I learn into the development practice in NBT Solutions.Among all of these, I would like to highlight the Open Data Discovery project, the monster consuming one third of my spare time!Like all the others who start their side projects, all I wanted to do initially was simply to make an open data map about data.gov. But it opened a world much border than I expect to me and there were so many to explore. This website ends upsupporting 3 major open data portal platforms: CKAN, DKAN, and Socratatracking more than 300 open data portals worldwideusing two AWS EC2 servers for website hosting and vector tile generationstoring ~1 million rows of data, which is open to downloadand more is under development. Scaling up a project feels great and the feedback looks great too.After doing this amount of work, it make me feels I am no longer a newbee knowing nothing. I am confident to walk out and reach out. In 2016 I attended two conferences, 2016 SIG/GIS Conference and 2016 NYS Geosptial Summit. I co-founded the MaptimeBUF, a local meetup for open source GIS, and gave the openning talk Introduce to OpenStreetMap. In December, I gave another talk, Make an Interactive Map with Leaflet, at the local JavaScript meetup MaptimeBUF.I spent my vocation at Washington DC and Ottawa, the two beautiful and historical capitals, and finished my city trip at northeast America. I watched my fist NFL game and the first star war movie. I started skiing as the winter came and got myself on more difficult trails :-)Finally I got the working VISA. Good luck 2016.","categories":[{"name":"Life","slug":"Life","permalink":"http://haoliangyu.github.io/blog/categories/Life/"}],"tags":[]},{"title":"The Launch of OpenDataDiscovery.org","slug":"The-Launch-of-OpenDataDiscovery.org","date":"2016-10-10T04:00:00.000Z","updated":"2018-03-03T20:45:36.118Z","comments":true,"path":"2016/10/10/The-Launch-of-OpenDataDiscovery.org/","link":"","permalink":"http://haoliangyu.github.io/blog/2016/10/10/The-Launch-of-OpenDataDiscovery.org/","excerpt":"After several months’ work at my spare time, I finally publish the first version of my side-project OpenDataDiscovery.org. It is a step to answer this very simple question in my mind: How many open datasets do we have on earth?","text":"After several months’ work at my spare time, I finally publish the first version of my side-project OpenDataDiscovery.org. It is a step to answer this very simple question in my mind: How many open datasets do we have on earth?I don’t remember exactly when I started thinking about this question. It seems to be natural to ask, but very easy to ignore at the era of open data blooming.Just look around. Every people, every government, every institute is talking about open data and releasing their data. EPA, NOAA, NOAA, and many other research institutes provide similar data from different angles about the same aspect of air pollution. Both USGS and USDA publish their soil survey. Numerous federal and local agencies are building their own data portals. Not to mention many research institutes have been doing that for years.The rapidly increasing number of data providers and open data creates a view of prosperity, which is going to create another problem: we are walking out of the data desert and now into a data jungle. When the available data is not as much as today, figuring out what and how data are published is not very difficult as there is a small market. However, within a data jungle, it may be easy to understand the status of data opening at the domain you are familiar with, but difficult to look beyond. It is not just because increasing open data brings increasing information, but there are more things happening outside our individual’s knowledge. We tend to be lost in the numerous options of data and miss the whole picture.Therefore, when I try to answer my initial question, I know there is a lot data at anywhere but cannot figure out even an estimate.The methodology to answer this question is not complex. I just need to collect information from all data portals and create the overview of the world of open data. The difficult part is all data portals. Fortunately, in the open data industry, there are dominant data portal platform providers, who are willing to open the API for data harvesting. These open API allows developers to collect rich information from each individual portal based on their platform, including dataset number, tag, category, publisher etc.All I need is a program that could help me automatically collect information from related portals and provide a summary view. So I develop the application OpenDataDiscovery.org, which runs the server for data collection and presents the map view of all data portals on earth.Today the OpenDataDiscovery.org has been up and running. It is collecting information for more than 100 CKAN instances (see the list) and presents the status for these portals. As it update in a weekly basis, it is able to keep track of the status of worldwide data opening.Like a puzzle game, it needs us to put every piece into the frame in order to retrieve the overall view. By continuously adding portals into OpenDataDiscovery.org, it will eventually present a full and live picture of the world of open data. Let’s see!","categories":[{"name":"GIS","slug":"GIS","permalink":"http://haoliangyu.github.io/blog/categories/GIS/"}],"tags":[{"name":"gis","slug":"gis","permalink":"http://haoliangyu.github.io/blog/tags/gis/"},{"name":"OpenDataDiscovery.org","slug":"OpenDataDiscovery-org","permalink":"http://haoliangyu.github.io/blog/tags/OpenDataDiscovery-org/"},{"name":"open data","slug":"open-data","permalink":"http://haoliangyu.github.io/blog/tags/open-data/"}]},{"title":"geojson-multiply: a simple package to pack single tyle geojson features","slug":"geojson-multiply-a-simple-library-to-pack-single-type-geojson-features","date":"2016-05-07T04:00:00.000Z","updated":"2018-03-03T20:45:36.138Z","comments":true,"path":"2016/05/07/geojson-multiply-a-simple-library-to-pack-single-type-geojson-features/","link":"","permalink":"http://haoliangyu.github.io/blog/2016/05/07/geojson-multiply-a-simple-library-to-pack-single-type-geojson-features/","excerpt":"Some PostGIS functions only accept multi type geometry, so I write a simple geojson utility package to help me aggregate geojson features.","text":"Some PostGIS functions only accept multi type geometry, so I write a simple geojson utility package to help me aggregate geojson features.The basic purpose of geojson-multiply is to generate a MutliPoint/MultiLineString/MultiPolygon geojson feature from many Point/LineString/Polygon geojson features. So this package provides a function1multiply(geojsons[, options])Where the geojsons could be a geojson feature, an array of geojson features, or a geojson feature collectionNot just the coordinates, the multiply() also supports the aggregation of properties. Its options parameter accepts two input:properties - the default properties of result geojsononEachFeature - a function to aggregate properties. It has four parameters:properties - the result geojson’s propertiesfeatureProp - input feature geojson’s propertiesindex - input feature geojson’s index in the arraygeojsons - geojson array.It takes the form of Array.reduce() and make the aggregation pretty straightforward:12345678910111213141516171819202122232425262728293031323334353637var multiply = require('geojson-multiply');var geojsonA = &#123; type: 'Feature', geometry: &#123; type: 'Point', coordinates: [12, 43] &#125;, properties: &#123; count: 5 &#125;&#125;;var geojsonB = &#123; type: 'Feature', geometry: &#123; type: 'Point', coordinates: [13, 34] &#125;, properties: &#123; count: 5 &#125;&#125;;var onEachFeature = function(properties, featureProp) &#123; properties.count += featureProp.count; return properties;&#125;;var result = multiply([geojsonA, geojsonB], &#123; properties: &#123; count: 0 &#125;, onEachFeature: onEachFeature&#125;);/**The reuslt geojson should be&#123; type: 'Feature' geometry: &#123; type: 'MultiPoint', coordinates: [[12, 43], [14, 34]] &#125;, properties: &#123; count: 10 &#125;&#125;*/This package has been published at npm. If you think it’s helpful, just install and try!1npm install geojson-multiply --save","categories":[{"name":"GIS","slug":"GIS","permalink":"http://haoliangyu.github.io/blog/categories/GIS/"}],"tags":[{"name":"geojson","slug":"geojson","permalink":"http://haoliangyu.github.io/blog/tags/geojson/"},{"name":"javascript","slug":"javascript","permalink":"http://haoliangyu.github.io/blog/tags/javascript/"}]},{"title":"Making Map with Leaflet in TypeScript","slug":"Making-A-Map-with-Leaflet-in-TypeScript","date":"2016-05-06T04:00:00.000Z","updated":"2018-03-03T20:45:36.118Z","comments":true,"path":"2016/05/06/Making-A-Map-with-Leaflet-in-TypeScript/","link":"","permalink":"http://haoliangyu.github.io/blog/2016/05/06/Making-A-Map-with-Leaflet-in-TypeScript/","excerpt":"UPDATE 27/12/2016: update for TypeScript 2.0Recently I am working on a web mapping project based on Angular 2. There isn’t much data on how to create web map using Leaflet in TypeScript. So I think it’s good to write a post on it.","text":"UPDATE 27/12/2016: update for TypeScript 2.0Recently I am working on a web mapping project based on Angular 2. There isn’t much data on how to create web map using Leaflet in TypeScript. So I think it’s good to write a post on it.Leaflet, TypedTypeScript is definitely typed so you need to provide a type declaration for Leaflet first. The installation of type declaration is much easier at TypeScript 2.0 for its direct support of npm and you just needs1234567891011/** * Type declaration for GeoJSON, a dependency */npm install @types/geojson/** * Type declaration for Leaflet */npm install @types/leafletIt will download the type declaration publish DefinitelyTyped and relate to the installed package. Simple and no more package manager!MappingUsing Leaflet in TypeScript doesn’t have an essential difference with using it in JavaScript. The only difference is, in the typed environment, many things are declared as class. So you may need to new a class before directly using it:1234let map = new L.Map('map', &#123; center: new L.LatLng(40.731253, -73.996139), zoom: 12,&#125;);Also you will need to provide extra type information when using function:123456map.on('click', (e: LeafletMouseEvent) =&gt; &#123; let marker = L.marker(e.latlng) .bindPopup('Popup') .addTo(map) .openPopup();&#125;);And that’s it. You should have no extra difficult in using Leaflet in TypeScript, if you are familiar with using it in JavaScript and has read the type declaration on what you need.Hope you enjoy the journey with TypeScript :)","categories":[{"name":"GIS","slug":"GIS","permalink":"http://haoliangyu.github.io/blog/categories/GIS/"}],"tags":[{"name":"leaflet","slug":"leaflet","permalink":"http://haoliangyu.github.io/blog/tags/leaflet/"},{"name":"typescript","slug":"typescript","permalink":"http://haoliangyu.github.io/blog/tags/typescript/"}]},{"title":"gtran: a promised, consistent, user-friendly GeoJson conversion package","slug":"gtran-a-promised-consistent-user-friendly-GeoJson-conversion-package","date":"2016-02-25T05:00:00.000Z","updated":"2018-03-03T20:45:36.138Z","comments":true,"path":"2016/02/25/gtran-a-promised-consistent-user-friendly-GeoJson-conversion-package/","link":"","permalink":"http://haoliangyu.github.io/blog/2016/02/25/gtran-a-promised-consistent-user-friendly-GeoJson-conversion-package/","excerpt":"Do you still feel inconvenient when converting GeoJson to other data formats in Node.js?","text":"Do you still feel inconvenient when converting GeoJson to other data formats in Node.js?GeoJson is one of simplest geographic data format. If we search for data conversion packages at npm.org, there will be a long list packages: shapefile for shapefile, tokml for kml, geojson2dcsv for csv, osm-and-geojson for osm package, geojson2 for multiple formats, org2org and gdal as data solutions… However, I still felt very inconvenient when I developed a GeoJson handling module a few months ago.Why inconvenient?From my point of view, there are a few problems:Most packages are designed for one specific data format.Each package is written in a style different from the other.Some packages require external libraries,like gdal, which aren’t npm-able.Most package aren’t asynchronous or promised. (maybe just a personal problem)As a result, in order to make them work together, one will need to learn how to use these diverse packages, customize code to handle their diverse input/output, and write some comments for the future project maintainer.Well, it’s not very fun.Gtran: the solutionThe programmer’s way to handle unhappiness is to write something that make him happy :) What I want is a package with following features:Able to work with multiple formatsConsistent and simple: functions and their input/outputCompletely npm-ableAsynchronousAnd here comes gtran.What does gtran do?gtran wraps several existing npm packages to providefrom/to conversion of multiple formats: .shp, .kml, .kmz, .csv, and TopoJson.consistent functions: from[format name]() and to[format name]()simple input/output: geojson or file nameasynchronous ability with your favorite promise library (bluebird, promise, Q, or native).minimal installation: each format support is provide by a child package gtran-xxx.It could be installed with npm install gtran.How could it be used?A complete use guide could be found at the GitHub repo and here is a small use case:12345678910111213141516171819202122232425262728293031var gtran = require('gtran');# Specify the promise library if necessarygtran.setPromiseLib(require('bluebird'));# Read shapefilegtran.fromShp('source.shp').then(function(object) &#123; var geojson = object;&#125;);# Save geojson into shapefilegtran.toShp(geojson, 'point.shp').then(function(fileNames) &#123; console.log('SHP files have been saved at:' + fileNames.toString());&#125;);# Read csv file# Assume the test.csv has two columns: latitude and longitudegtran.fromCSV('source.csv', &#123; mapping: &#123; x: 'longitude', y: 'latitude' &#125;&#125;).then(function(object) &#123; var geojson = object;&#125;);# Save geojson into a csv filegtran.toCSV(geojson, 'point.csv').then(function(fileName) &#123; console.log('CSV file has been saved at:' + fileName);&#125;);Hope you enjoy it. If you find a bug or want a new feature, just create an issue and I will appreciate it :-)","categories":[{"name":"Project","slug":"Project","permalink":"http://haoliangyu.github.io/blog/categories/Project/"}],"tags":[{"name":"gis","slug":"gis","permalink":"http://haoliangyu.github.io/blog/tags/gis/"},{"name":"project","slug":"project","permalink":"http://haoliangyu.github.io/blog/tags/project/"},{"name":"javascript","slug":"javascript","permalink":"http://haoliangyu.github.io/blog/tags/javascript/"}]},{"title":"Lazy man's package of nationwide U.S. boundaries","slug":"Lazy-man-s-package-of-U-S-boundaries","date":"2015-12-05T05:00:00.000Z","updated":"2016-01-24T05:00:00.000Z","comments":true,"path":"2015/12/05/Lazy-man-s-package-of-U-S-boundaries/","link":"","permalink":"http://haoliangyu.github.io/blog/2015/12/05/Lazy-man-s-package-of-U-S-boundaries/","excerpt":"I have been doing GIS work for quite a while and I do hate to download data from multiple sites every time. So I decide to collect those commonly geospatial data and make my own database. Of course, like all open-source people do, I’d love to share my work with you. The first release is the U.S. nationwide U.S. boundary. This dataset include nation, state, county, place, census block, and census tract. The source data come from U.S. Census Bureau and you can find the metadata here.Multiple formats provided: shapefile, geojson, and a postgis database dump.","text":"I have been doing GIS work for quite a while and I do hate to download data from multiple sites every time. So I decide to collect those commonly geospatial data and make my own database. Of course, like all open-source people do, I’d love to share my work with you. The first release is the U.S. nationwide U.S. boundary. This dataset include nation, state, county, place, census block, and census tract. The source data come from U.S. Census Bureau and you can find the metadata here.Multiple formats provided: shapefile, geojson, and a postgis database dump.shapefile/geojsonResolution1:500,0001:5,000,0001:20,000,000NationShapefile/GeoJsonShapefile/GeoJsonStateShapefile/GeoJsonShapefile/GeoJsonShapefile/GeoJsonCountyShapefile/GeoJsonShapefile/GeoJsonShapefile/GeoJsonCensus TractShapefile/GeoJsonCensus BlockShapefile/GeoJsonPlaceShapefile/GeoJsonpostgisDownload U.S. boundary databaseThe data links are also at GitHub.Ok, enjoy the data :)","categories":[{"name":"GIS","slug":"GIS","permalink":"http://haoliangyu.github.io/blog/categories/GIS/"}],"tags":[{"name":"gis","slug":"gis","permalink":"http://haoliangyu.github.io/blog/tags/gis/"},{"name":"open data","slug":"open-data","permalink":"http://haoliangyu.github.io/blog/tags/open-data/"}]},{"title":"Generating masks from Landsat 8 image in ArcMap","slug":"Generating-masks-from-Landsat-8-image-in-ArcMap","date":"2015-05-09T01:38:02.000Z","updated":"2018-03-03T20:45:36.118Z","comments":true,"path":"2015/05/09/Generating-masks-from-Landsat-8-image-in-ArcMap/","link":"","permalink":"http://haoliangyu.github.io/blog/2015/05/09/Generating-masks-from-Landsat-8-image-in-ArcMap/","excerpt":"Two months ago I have written a small python package pymasker to generate mask from the Quality Assessment band of Landsat 8 image and MODIS land products. This package is gaining popularity, But scripting may not be convenient for users who are not familiar with programming, therefore I create a ArcMap python toolbox based on this package for interactive masking.","text":"Two months ago I have written a small python package pymasker to generate mask from the Quality Assessment band of Landsat 8 image and MODIS land products. This package is gaining popularity, But scripting may not be convenient for users who are not familiar with programming, therefore I create a ArcMap python toolbox based on this package for interactive masking.The ArcMasking toolbox contains two script tools, one designed for Landsat 8 QA band and one for generic quality assessment bits in other NASA remote sensing products like MODIS.This tool can be download at GitHub. After unzipping the package, you could open the ArcMasker.tbx file at the Catalog window in ArcMap and find following tools.From Landsat 8 QA BandThis tool is used to create masks from the Quality Assessment band of Landsat 8 OLI image. It requires several inputs:Quality Assessment Band of Landsat 8 imageMask type including cloud, cirrus, water, snow or vegetationConfidence Level that indicates the likelihood of existing of specific situation. You can also choose to include confidence level above what has been selected.Output Mask saving pathFrom Quality Assessment BitsThis tool is used to create masks from other NASA remote sensing products with general quality assessment bit like MODIS land products. As each portion of QA bits suggests a certain condition of the image, this toolbox simply examines the value of specific portion of QA bits and generates mask accordingly. Required inputs include:Quality Assessment BandBit Position that indicates where the desired portion of bits starts.Bit Length that indicates the length of bits.Bit Value that indicates the desired situation.Output Mask saving path","categories":[{"name":"GIS","slug":"GIS","permalink":"http://haoliangyu.github.io/blog/categories/GIS/"}],"tags":[{"name":"arcgis","slug":"arcgis","permalink":"http://haoliangyu.github.io/blog/tags/arcgis/"},{"name":"remote sensing","slug":"remote-sensing","permalink":"http://haoliangyu.github.io/blog/tags/remote-sensing/"},{"name":"landsat","slug":"landsat","permalink":"http://haoliangyu.github.io/blog/tags/landsat/"}]},{"title":"Converting C# Data Value to ArcObject Raster Pixel Value","slug":"Converting-C-Data-Value-to-ArcObject-Raster-Pixel-Value","date":"2015-03-22T01:30:54.000Z","updated":"2018-03-03T20:45:36.114Z","comments":true,"path":"2015/03/22/Converting-C-Data-Value-to-ArcObject-Raster-Pixel-Value/","link":"","permalink":"http://haoliangyu.github.io/blog/2015/03/22/Converting-C-Data-Value-to-ArcObject-Raster-Pixel-Value/","excerpt":"The length of remote sensing pixel value is constrained by a predefined pixel value type. It is an unchangeable property for each remote sensing image and saving an unsupported value to a image may cause unpredictable errors like application crash or information loss resulted from implicit data type conversion.","text":"The length of remote sensing pixel value is constrained by a predefined pixel value type. It is an unchangeable property for each remote sensing image and saving an unsupported value to a image may cause unpredictable errors like application crash or information loss resulted from implicit data type conversion.If you are developing application based on ArcObject, you could know the pixel type of a raster layer simply with following code.12345IRasterLayer rasterLayer = (IRasterLayer)layer;IRasterProps rasterProps = (IRasterProps)rasterLayer.Raster;// Get the pixel type of you raster layerrstPixelType pixelType = rasterProps.PixelType;You can find a full list and detailed explanation of ArcObject raster pixel types in this page.Not every raster pixel types has a corresponding C# value type. So I make a list for those pixels types having relative in C# so that we can make correct conversion while developing.ArcMap Pixel TypeDescriptionCorresponding C# Data TypeSupportedPT_UNKNOWNunknownNoPT_U11 bitNoPT_U22 bitNoPT_U44 bitNoPT_UCHARunsigned 8 bit integerByteYesPT_CHAR8 bit integerSByteYesPT_USHORTunsigned 16 bit integerUInt16YesPT_SHORT16 bit integerInt16YesPT_ULONGunsigned 32 bit integerUInt32YesPT_LONG32 bit integerInt32YesPT_FLOATsingle precision floating pointSingleYesPT_DOUBLEdouble precision floating pointDoubleYesPT_COMPLEXsingle precision complexNoPT_DCOMPLEXdouble precision complexNoPT_CSHORTshort integer complexNoPT_CLONGlong integer complexNoBased on this table, I write a function to wrap the conversion from C# data type to valid ArcObject pixel value type. I use it in my project ArcMap Raster Edit Suite and it work pretty well.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/// &lt;summary&gt;/// Convert the csharp value to the ArcObject pixel value./// &lt;/summary&gt;/// &lt;param name=\"csharpValue\"&gt;Cshapr value&lt;/param&gt;/// &lt;param name=\"pixelValueType\"&gt;The pixel type of ouput value&lt;/param&gt;/// &lt;param name=\"pixelValue\"&gt;Output pixel value&lt;/param&gt;/// &lt;returns&gt;A value indicating whether the convention is successful&lt;/returns&gt;public static bool CSharpValue2PixelValue(object csharpValue, rstPixelType pixelValueType, out object pixelValue)&#123; try &#123; switch (pixelValueType) &#123; case rstPixelType.PT_UCHAR: pixelValue = (object)Convert.ToByte(csharpValue); return true; case rstPixelType.PT_CHAR: pixelValue = (object)Convert.ToSByte(csharpValue); return true; case rstPixelType.PT_SHORT: pixelValue = (object)Convert.ToInt16(csharpValue); return true; case rstPixelType.PT_USHORT: pixelValue = (object)Convert.ToUInt16(csharpValue); return true; case rstPixelType.PT_CLONG: pixelValue = (object)Convert.ToInt32(csharpValue); return true; case rstPixelType.PT_ULONG: pixelValue = (object)Convert.ToUInt32(csharpValue); return true; case rstPixelType.PT_FLOAT: pixelValue = (object)Convert.ToSingle(csharpValue); return true; case rstPixelType.PT_DOUBLE: pixelValue = (object)Convert.ToDouble(csharpValue); return true; default: pixelValue = null; return false; &#125; &#125; catch (Exception) &#123; pixelValue = null; return false; &#125;&#125;","categories":[{"name":"GIS","slug":"GIS","permalink":"http://haoliangyu.github.io/blog/categories/GIS/"}],"tags":[{"name":"arcobject","slug":"arcobject","permalink":"http://haoliangyu.github.io/blog/tags/arcobject/"},{"name":"C#","slug":"C","permalink":"http://haoliangyu.github.io/blog/tags/C/"}]},{"title":"Yet another way to edit your raster layer in ArcMap - Paint on it!","slug":"Yet-another-way-to-edit-your-raster-layer-in-ArcMap-Paint-on-it","date":"2015-03-12T01:44:53.000Z","updated":"2018-03-03T20:45:36.134Z","comments":true,"path":"2015/03/12/Yet-another-way-to-edit-your-raster-layer-in-ArcMap-Paint-on-it/","link":"","permalink":"http://haoliangyu.github.io/blog/2015/03/12/Yet-another-way-to-edit-your-raster-layer-in-ArcMap-Paint-on-it/","excerpt":"I am glad to announce that the project ArcMap Raster Edit Suite (ARES) has proceed to its second major version 0.2.0! A completely new toolbar Raster Painter is added into the add-in. Many people have realized that editing pixels using tools in Raster Editor may be labor intensive, especially when editing large number of pixels. This toolbar is here to provide an ultimately flexible solution to edit pixels, by painting new values on it. The editing style is design to be similar to draw image and picture. You can find similar toolbars in ArcMap like ArcScan and Raster Painting, but they are not designed for raster data modification. Of course, a very close analogy is Paint on Window.","text":"I am glad to announce that the project ArcMap Raster Edit Suite (ARES) has proceed to its second major version 0.2.0! A completely new toolbar Raster Painter is added into the add-in. Many people have realized that editing pixels using tools in Raster Editor may be labor intensive, especially when editing large number of pixels. This toolbar is here to provide an ultimately flexible solution to edit pixels, by painting new values on it. The editing style is design to be similar to draw image and picture. You can find similar toolbars in ArcMap like ArcScan and Raster Painting, but they are not designed for raster data modification. Of course, a very close analogy is Paint on Window.Raster Painter ToolbarThe ARES installation package could be downloaded at the GitHub project page.After installing the ARES Add-In, a new toolbar Raster Painter will be added into ArcMap.In the initial release, the tool includes two basic tools:Freehand Paint - provide a freehand painting on the raster layerErase - Erase unsaved painted pixels from cacheThis toolbar aims at maximizing the flexibility, not the accuracy (e.g. editing pixels at specific rows and columns). If the accuracy is your need, please have a look at another toolbar Raster Editor.Painting on the layerPainting on a raster layer in ArcMap is just like painting a picture with Paint or PhotoShop. The major difference is just you are painting values, instead of colors, on the raster layer. You can do it just with a few steps.Start Painting Section. You can only paint on a raster layer for one time. If you have several layers in ArcMap, you will select one for painting in the selection window.Add Values for Painting. Click the Add Values button on the Raster Paint window, in order to add values that you want to paint on the layer.A random color will be assigned to each added value. You can change the color by right clicking the color column on the value list.If you want to delete values from the list, just click the Delete Values button.If you want to add new values that does not exist on the layer, click Add New Values… button on the Options Menus. However, you are not able to add any value violating the pixel type of the layer (e.g. 257 for a 8-bit unsigned integer layer).Painting Values. After selecting a value on the Raster Paint window, you can paint the value on the layer using the Freehand Paint tool.While painting, please keep you mouse move smoothly and steadily.Painted values are symbolized with yellow frame so that you are not going to miss them.Erase Painted Values. If you want to remove values you paint, just use the Erase tool to remove them.Noted that the Erase tool can only remove painted values on the layer. If you want to erase existing values on the layer, you could simply paint the NoData value on pixels.Save Painted Values. You can save painted values to the original file using the Save Paints button, or to a new file using the Save Paints As button.Stop Painting SectionNow you have your raster layer edited. Isn’t it simple and quick :)What is next?The initial release only provides the most basic functionality. More advanced and handy tools are under development:Draw polyline/polygon/Circle/RectangleDraw in selection regionFill polygonPainting history (e.g. undo and redo)I hope I can finish these updates soon. If you have any idea about the new functionality, don’t hesitate to give me a comment :)Special ThanksI would like to thank Xuan Wang, Hancheng Nie and Jiang Qing for contributing their codes to this project. And I would like to thank all people who support and encourage me to continue this project.","categories":[{"name":"GIS","slug":"GIS","permalink":"http://haoliangyu.github.io/blog/categories/GIS/"}],"tags":[{"name":"gis","slug":"gis","permalink":"http://haoliangyu.github.io/blog/tags/gis/"},{"name":"arcgis","slug":"arcgis","permalink":"http://haoliangyu.github.io/blog/tags/arcgis/"}]},{"title":"Making masks from Quality Control bits of MODIS land products in Python (Update)","slug":"Making-masks-from-Quality-Control-bits-of-MODIS-land-products-in-Python-Update","date":"2015-02-20T02:23:29.000Z","updated":"2018-03-03T20:45:36.118Z","comments":true,"path":"2015/02/20/Making-masks-from-Quality-Control-bits-of-MODIS-land-products-in-Python-Update/","link":"","permalink":"http://haoliangyu.github.io/blog/2015/02/20/Making-masks-from-Quality-Control-bits-of-MODIS-land-products-in-Python-Update/","excerpt":"Update Since 0.3.0, most class and function names have been updated according to pep8.Days ago when I published the first version of pymasker for masking Landsat 8 image, Dr. Robert A. Washington-Allen suggested me whether the package would be used for MODIS products. After I read technical documents of MODIS, I realize that the Quality Control bits of MOIDS products has the same structure as the QA band of Landsat 8. Therefore, I update the package to adapt MODIS products (or other NASA products with similar QC/QA bits).","text":"Update Since 0.3.0, most class and function names have been updated according to pep8.Days ago when I published the first version of pymasker for masking Landsat 8 image, Dr. Robert A. Washington-Allen suggested me whether the package would be used for MODIS products. After I read technical documents of MODIS, I realize that the Quality Control bits of MOIDS products has the same structure as the QA band of Landsat 8. Therefore, I update the package to adapt MODIS products (or other NASA products with similar QC/QA bits).In this article, I am going to show how to extract masks from the Quality Control bits of MODIS land product in python.MODIS QC/QA bitsThe QC/QA bits of MODIS is a binary number, of which each section represents the state of certain condition. Just take the 250m resolution land surface reflectance product (MOD09GQ) as example. The QC band (maybe called QA band in other products) is the fourth band of the HDF dataset. It is a unsigned 16-bit band. Each value on the pixel indicates a combination of pixel conditions. We can interpret the value by separating its binary form.The individual bits within a binary number are read from let to right as described in the following table.Therefore when we put the interpretation back to the bit string, we are able to know what it indicates.As this binary number is 4096 in integer, all pixels having value 4096 in the QC band are high quality and cloud free with atmosphere correction, but no adjacency correction.The structure of QC/QA bits vary in different MODISO products and different data collection. For full detail for each product, please have a look atMODIS Land Products QA TutorialIn the next sections, I will show how to make masks based on the QC/QA bits in python.PreparationBefore getting started, you need to do the following preparations:Download and install pymasker.Know the band number of QC/QA band in your HDF dataset.Know the QC/QA bit structure of your dataset. If you only want the mask of quality level, you don’t have to know the bit structure. Please go bottom and read the new update.Maksing for MODIS productI will continue to use MOD09GQ in the sample. First you need to load the QC Band12345678910111213from pymasker import Masker# Directly load from file, require GDAL (3 is the QA band number)masker = Masker('MOD09GQ.A2015025.h12v04.005.2015027064556.hdf', 3)# Or you just load the dataset somewhere and get the band data as numpy arrayimport gdalhdfdataset = gdal.Open('MOD09GQ.A2015025.h12v04.005.2015027064556.hdf')subdataset = hdfdataset.GetSubDatasets()[3][0]bandarray = gdal.Open(subdataset).ReadAsArray()masker = Masker(bandarray)You need to know the bit information to generate certain masks. For example, we want a mask only wit high quality pixels. In the previous section we have known that the quality bits start at the beginning (position 0) and the length is 2. We also know that ‘00’ indicate corrected product produced at ideal quality in all bands. Therefore, the high quality mask could be made with12345# Get high quality maskmask = modismasker.ge_tmask(bitpos = 0, bitlen = 2, value = '00')# Save the mask as a TIF filemasker.save_tif(hqmask, 'result.tif')Then you can get a binary mask where 1 represents the high quality pixel.UpdatePyamsker has provided a new class for producing QA mask since version 0.2.2. So you don’t have to look for and remember those binary bits (Well I don’t like them, too).I have wrapped quality levels for MODIS land products.12345678910111213from pymasker import ModisQuality# Corrected product produced at ideal quality for all bands.quality = ModisQuality.high# Corrected product produced at less than ideal quality for some or all bands.quality = ModisQuality.medium# Corrected product not produced due to some reasons for some or all bands.quality = ModisQuality.low# Corrected product not produced due to cloud effects for all bands.quality = ModisQuality.low_cloudTherefore, the masking code would be very intuitive and readable.12345# Create a MODIS QA masker, similar to creating a masker abovefrom pymasker import ModisMaskermasker = ModisMasker('MOD09GQ.A2015025.h12v04.005.2015027064556.hdf')mask = masker.get_qa_mask(ModisQuality.high)I am going to gradually support all MODIS products in the future release. Please look forward to my new update :)","categories":[{"name":"GIS","slug":"GIS","permalink":"http://haoliangyu.github.io/blog/categories/GIS/"}],"tags":[{"name":"remote sensing","slug":"remote-sensing","permalink":"http://haoliangyu.github.io/blog/tags/remote-sensing/"},{"name":"modis","slug":"modis","permalink":"http://haoliangyu.github.io/blog/tags/modis/"},{"name":"python","slug":"python","permalink":"http://haoliangyu.github.io/blog/tags/python/"}]},{"title":"Editing single pixels of raster layer in ArcMap with just a few clicks","slug":"Editing-single-pixels-of-raster-layer-in-ArcMap-with-just-a-few-clicks","date":"2015-02-18T02:42:56.000Z","updated":"2018-03-03T20:45:36.114Z","comments":true,"path":"2015/02/18/Editing-single-pixels-of-raster-layer-in-ArcMap-with-just-a-few-clicks/","link":"","permalink":"http://haoliangyu.github.io/blog/2015/02/18/Editing-single-pixels-of-raster-layer-in-ArcMap-with-just-a-few-clicks/","excerpt":"Editing single pixels of a raster layer is a common task in daily GIS and RS practice. For exampleRemoving misclassified pixels from a land cover classification resultCreating a set of multiple raster layers with minor difference for model simulationetc.In ArcMap we use Raster Calculator and Reclassification tool as many instructions suggest. However, these tools are designed to edit the layer as a whole and they are not flexible enough for minor modification. If ArcMap allows us to edit single features of a vector layer, why we cannot edit single pixels of a raster layer in ArcMap? That is the reason why the project ArcMap Raster Edit Suite (ARES) starts. It aims at providing an ArcMAp Add-In with tools to allows single raster pixels editing, just like what we can do with the vector layer. Flexible, quick and neat!","text":"Editing single pixels of a raster layer is a common task in daily GIS and RS practice. For exampleRemoving misclassified pixels from a land cover classification resultCreating a set of multiple raster layers with minor difference for model simulationetc.In ArcMap we use Raster Calculator and Reclassification tool as many instructions suggest. However, these tools are designed to edit the layer as a whole and they are not flexible enough for minor modification. If ArcMap allows us to edit single features of a vector layer, why we cannot edit single pixels of a raster layer in ArcMap? That is the reason why the project ArcMap Raster Edit Suite (ARES) starts. It aims at providing an ArcMAp Add-In with tools to allows single raster pixels editing, just like what we can do with the vector layer. Flexible, quick and neat!Download and InstallThis project is hosted and published at GitHub. You can download the installation file at HereAfter the download finishes, you need to choose the installation file that matches the ArcMap on your computer because this Add-In support ArcMap 10.0/10.1/10.2. There are two files in the folder and ARES.esriAddin is what you need:Double click and you could see the installation wizard.Just click Install Add-In and then you get it.Raster Editor ToolbarNow you have a new toolbar Raster Editor installed in your ArcMap.This toolbar includes following tools:: Select &amp; Edit, which is used to select pixels for editing.: Identify, which is used to select pixels in order to get their values, row/column indexes and zonal statistics.: Go To Pixel, which is used to locate pixel with given row and column index.The purpose of this toolbar is to provide tools for precise modification with known row and column index.Step by Step: Editing Raster LayerIn this section, I am going to show a brief guide on how to edit single pixels of raster layer in ArcMap.First of all, we have the layer imported in ArcMap.Then start the editing section at the Start Editor menu by clicking Start Editing button. You can only edit one raster layer at one time. If there are more than one raster layers in ArcMap, there will be a prompt-up window to let you choose one.Click the Select &amp; Edit button to activate the tool.Click or drag a region of interest on the layer to selected pixels. All selected pixels will be highlighted on the map with a blue frame. Noted that selecting too much pixels may lead to crash of ArcMap.Edit values of selected pixel at the Raster Edit Window. All edits will be highlighted on the map with a yellow symbol.Make sure your input value is compatible with the pixel type of the raster layer. Any pixel left blank will be considered as NoData Pixel.Save edits to the raster file using Save Edits button on the Raster Editor menu. If you want to save edits to a new file, click the Save Edits As button. You will see the change of layer right after refreshing the ArcMap.Stop the editing section by clicking Stop Editing button.Now you finish the editing section and get the desired result.Why there is only one toolbar in the suite?Well, we are working on anther toolbar so that we can make it a real tool suite. Please have a visit on our project page later.If you like this project, give it a star :)","categories":[{"name":"GIS","slug":"GIS","permalink":"http://haoliangyu.github.io/blog/categories/GIS/"}],"tags":[{"name":"gis","slug":"gis","permalink":"http://haoliangyu.github.io/blog/tags/gis/"},{"name":"arcgis","slug":"arcgis","permalink":"http://haoliangyu.github.io/blog/tags/arcgis/"}]},{"title":"Comparison of different sampling rules in SVM classification","slug":"Comparison-of-different-sampling-rules-in-SVM-classification","date":"2015-01-26T05:00:00.000Z","updated":"2018-03-03T20:45:36.114Z","comments":true,"path":"2015/01/26/Comparison-of-different-sampling-rules-in-SVM-classification/","link":"","permalink":"http://haoliangyu.github.io/blog/2015/01/26/Comparison-of-different-sampling-rules-in-SVM-classification/","excerpt":"The problem of sampling rule in Support Vector Machine raised in the discussion when I was in the course Advanced Remote Sensing.The quality of training sample has deterministic influence to the accuracy of supervised classification methods. A properly designed sampling rule plays a substantial role in yielding an informative training sample set. The design of sampling strategy depends on both the characteristics of objects of interest and the applied classification methods.","text":"The problem of sampling rule in Support Vector Machine raised in the discussion when I was in the course Advanced Remote Sensing.The quality of training sample has deterministic influence to the accuracy of supervised classification methods. A properly designed sampling rule plays a substantial role in yielding an informative training sample set. The design of sampling strategy depends on both the characteristics of objects of interest and the applied classification methods.Support Vector Machine (SVM) is an increasingly popular classification method for remote sensing image both in industry and research. As for the theory and concept of SVM, there have been many introductory articles such as Roemer`s blog. Because support vectors are selected from training samples for maximizing the margin between classes, quality of sample set will decide the effectiveness of the hyperplane and affect the performance of the classifier.In brief the mechanism of SVM is very different from traditional methods like Maximum Likelihood Classification. Here comes the question: Does the sampling rule used in traditional methods also work for SVM? How does the sampling rule affect the SVM classification result?ExperimentTo answer this question, I would like to set up a scenario for the classification experiment. In this case, I choose to perform a cloud detection based on Landsat 8 OLI image for two reasons:It is a most basic binary classification.Cloud detection could be very challenging because thin cloud is very difficult to detect in many situations.I use an image with both thick cloud and thin cloud in order to increase the difficulty of classification (See Figure 1). Band 2 to 7 are used and the SVM classifier in ENVI is utilized with default configuration. Four hundred pixels are randomly picked up as test sample: half of them are cloud (blue points) and the others are ground objects (yellow points).Sampling RulesI am particularly interested in three sample rules.Stratified random samplingThis is a classic sample rule. Its idea is to randomly select training samples of one class from the sample pool of that class. The sample size of each class could be a fixed number or proportional to the size of sample pool of that class.Cluster samplingHowever, in practice random sampling is sometimes difficult to achieve, especially when the sample is selected by hand or a semi-automatic method. The sample pixels generally tend to be more clustering than random. (it is actually what we do in the ENVI class: drag a region of interest and treat it as sample set)Boundary samplingIts basic idea is that support vectors are likely also located near the geographical boundary of two classes. Although pixels located around geographic boundaries might contain spectral information of both classes to some extent, Foody and Mathur (2006) suggested that mixing pixels at two sides of ground object boundary could be highly effective in SVM even the sample size was very smaller. They considered this approach as a competitive alternative of conventional sampling strategy.So I select three training sets based on above sampling rules (See Figure 2).Training set in feature spaceWhile using the SVM, we hope that our training samples are located near the boundary of classes in the feature space. Figure 3 shows how samples in three training sets, as well as the test set, scatter at a feature space (red band vs. NIR band).Cloud has high reflectance in both red band and near infrared band, however, the reflectance of ground objects (most are vegetation) is low in red band and high in near infrared band. Due to the reflectance difference, samples in the feature space apparently belong to two distinct groups: cloud and ground objects. As shown in Figure 3, the distribution of samples in different training sets are totally different. Boundary sampling produced samples located near the boundary of two classes. Samples generated with stratified random sampling could better represent the overall characteristics of two classes. Clustering sampling within high confidence area would generate samples also clustering in the feature space.ResultThe classification results are shown in Table 1 and Figure 4.The results, as expected, indicate that boundary sampling is able to generate better training sample and contribute to higher classification accuracy, compared to the other two sampling strategies.ALl trained classifiers are also applied to the whole study image. Classifier trained with boundary samples is very sensitive to the difference between cloud and non-cloud pixels. It will try to differentiate all mixing pixels and this could be too sensitive. Classifier trained by cluster samples only recognizes thick cloud on the image and ignores pixels with mixing spectral information. Classifier trained by stratified random samples takes a balance between them: neither too aggressive nor too conventional. For detailed discussion, please read my report (see link below).ConclusionLet’s get back to our initial questions and their answers.Does the sampling rule used in traditional methods also work for SVM?Apparently the traditional stratified random sampling still works pretty well for the SVM classification. The classification accuracy in the experiment is about 90%, which is high enough for most situations.How does the sampling rule affect the SVM classification result?If we can use the portion of boundary (mixing) pixels in the training set to define sampling rule, we are able to build a link among three sampling rules used above. As we include more boundary pixels into the training set, the classifier would become more sensitive and have a better performance in identifying mixing objects. However, high sensitivity could also increase the error rate while identifying mixing objects. There is a trade-off between high sensitivity and low error rate within the selection of sampling strategies.There is no generally appropriate sampling rule for all situations. However, depending on the need of classification, there exits an appropriate sampling rule for specific situation. Therefore, it is not difficult to understand why the stratified random sampling is still the first choice in practical use. Though classifier trained stratified random sampling is expected to have fair performance, it is less likely to be trapped into the problem of overestimation and underestimation.LinkThis article is a simplified version of my project report. For more detail, please read my full report A Comparison of Sampling Strategies in SVM Classification.Reference[1] Foody, G. M. and A. Mathur (2006). “The use of small training sets containing mixed pixels for accurate hard image classification: Training on mixed spectral responses for classification by a SVM.” Remote Sensing of Environment 103(2): 179-189.","categories":[{"name":"GIS","slug":"GIS","permalink":"http://haoliangyu.github.io/blog/categories/GIS/"}],"tags":[{"name":"remote sensing","slug":"remote-sensing","permalink":"http://haoliangyu.github.io/blog/tags/remote-sensing/"}]},{"title":"Pymasker","slug":"Pymasker","date":"2015-01-22T03:03:02.000Z","updated":"2018-03-03T20:45:36.118Z","comments":true,"path":"2015/01/22/Pymasker/","link":"","permalink":"http://haoliangyu.github.io/blog/2015/01/22/Pymasker/","excerpt":"Pymasker is a python package to generate various masks from the landsat 8 Quality Assessment band and MODIS products.","text":"Pymasker is a python package to generate various masks from the landsat 8 Quality Assessment band and MODIS products.This project is hosted at GitHub.InstallationThe package can be shipped to your computer using pip.1pip install pymaskerOr just install it with the source code.1python setup.py installThis package depends on numpy. If you want to directly load the QA band file, GDAL is also in need if you want to .Releasse0.1.0First release of this package0.1.2Added multi-criteria maskingAdded new confidence leve: none0.1.3Added cumulative options for multi-criteria maskingFixed a default value problem in multi-criteria masking0.1.4Removed the dependence of GDAL0.2.0Added support for MODIS land products0.2.3Restructure package","categories":[{"name":"Project","slug":"Project","permalink":"http://haoliangyu.github.io/blog/categories/Project/"}],"tags":[{"name":"gis","slug":"gis","permalink":"http://haoliangyu.github.io/blog/tags/gis/"},{"name":"project","slug":"project","permalink":"http://haoliangyu.github.io/blog/tags/project/"},{"name":"python","slug":"python","permalink":"http://haoliangyu.github.io/blog/tags/python/"}]},{"title":"Making masks with Landsat 8 Quality Assessment band using Python","slug":"Making-masks-with-Landsat-8-Quality-Assessment-band-using-Python","date":"2015-01-19T02:13:06.000Z","updated":"2018-03-03T20:45:36.118Z","comments":true,"path":"2015/01/19/Making-masks-with-Landsat-8-Quality-Assessment-band-using-Python/","link":"","permalink":"http://haoliangyu.github.io/blog/2015/01/19/Making-masks-with-Landsat-8-Quality-Assessment-band-using-Python/","excerpt":"Update Since 0.2.0, the qabmasker has been renamed as landsatmasker and the function getmask has been renamed as getmultimask for better identification. Please update your script accordingly.Update Since 0.3.0, most class and function names have been updated according to the pep8.The Quality Assessment (QA) band is a standard product included in each Landsat 8 image downloaded from USGS, which contains the quality information of each pixel such as cloud, snow/ice, water body etc. The detailed information of this band can be found at Landsat 8 Quality Assessment Band.","text":"Update Since 0.2.0, the qabmasker has been renamed as landsatmasker and the function getmask has been renamed as getmultimask for better identification. Please update your script accordingly.Update Since 0.3.0, most class and function names have been updated according to the pep8.The Quality Assessment (QA) band is a standard product included in each Landsat 8 image downloaded from USGS, which contains the quality information of each pixel such as cloud, snow/ice, water body etc. The detailed information of this band can be found at Landsat 8 Quality Assessment Band.Although the bits structure of the QA band is easy-to-understand, it would several steps to extract the desired mask in QGIS or L-LDOPE Toolbelt. In this article, I am going to introduce how to use pymasker to generate masks from Landsat 8 QA band in Python. This package is designed to provide a straightforward and intuitive way to generate masks from the QA band. It is an open-source package and the project is hosted at GitHub.InstallationThe package can be shipped to your computer using pip.1pip install pymaskerOr just install it with the source code.1python setup.py installThis package depends on numpy. If you want to directly load the QA band file, GDAL is also in need.SampleFirst of all, you need to load the QA band into the qabmaser class.1234567from pymasker import LandsatMasker# load the QA band directlymasker = LandsatMasker('LC80170302014272LGN00_BQA.TIF')# load a numpy array that contains band datamasker = LandsatMasker(bandarray)The QA band contains the detection result of the following four specific conditions for each pixel:CloudCirrusSnow/IceVegetationWater bodyFor each condition, the algorithm gives a confidence to indicate its existence on the pixel.12345678910111213141516from pymasker import LandsatConfidence# Algorithm has high confidence that this condition exists (67-100 percent confidence).conf = LandsatConfidence.high# Algorithm has medium confidence that this condition exists (34-66 percent confidence).conf = LandsatConfidence.medium# Algorithm has low to no confidence that this condition exists (0-33 percent confidence)conf = LandsatConfidence.low# Algorithm did not determine the status of this condition.conf = LandsatConfidence.undefined# Nothing, unspecified confidenceconf = LandsatConfidence.noneTo generate a mask, you need to define a desired confidence for the target condition.Pymasker provides several functions to get the most commonly used mask. The resulting mask is a binary numpy array with 1 representing existence of specific condition and 0 representing nonexistence.1234567891011121314# Get mask indicating cloud pixels with high confidencemask = masker.get_cloud_mask(LandsatConfidence.high)# Get mask indicating cloud pixels with at least medium confidencemask = masker.get_cloud_mask(LandsatConfidence.medium, cumulative = True)# Get mask indicating snow/ice pixels with at least medium confidencemask = masker.get_snow_mask(LandsatConfidence.medium, cumulative = True)# Get mask indicating water body pixels with high confidencemask = masker.get_water_mask(LandsatConfidence.high)# Get mask indicating vegetation pixels with high confidencemask = masker.get_veg_mask(LandsatConfidence.high)Pymasker also provides a function for multi-criteria masking. In this function, you need to specify the confidence level of each condition. If you don’t want the function consider one of conditions, you need to set it as confidence.none. Two different masking methods are provided:Inclusive - Mask pixels that meet at least one of all criteria.Exclusive - Only Mask pixels that meet all criteria. (default)Sample code for multi-criteria masking123456# Get mask indicating cloud pixels (high confidence) and cirrus pixels (high confidence).# Exclusive and noncumulativemask = masker.get_multi_mask(cloud = LandsatConfidence.high, water = LandsatConfidence.high)# Save the result if you want.masker.save_tif(mask, 'result.tif')Now you have your mask! Nice and quick!","categories":[{"name":"GIS","slug":"GIS","permalink":"http://haoliangyu.github.io/blog/categories/GIS/"}],"tags":[{"name":"remote sensing","slug":"remote-sensing","permalink":"http://haoliangyu.github.io/blog/tags/remote-sensing/"},{"name":"landsat","slug":"landsat","permalink":"http://haoliangyu.github.io/blog/tags/landsat/"},{"name":"python","slug":"python","permalink":"http://haoliangyu.github.io/blog/tags/python/"}]},{"title":"ArcMap Raster Edit Suite","slug":"ArcMap-Raster-Edit-Suite","date":"2014-09-17T04:00:00.000Z","updated":"2015-12-05T05:00:00.000Z","comments":true,"path":"2014/09/17/ArcMap-Raster-Edit-Suite/","link":"","permalink":"http://haoliangyu.github.io/blog/2014/09/17/ArcMap-Raster-Edit-Suite/","excerpt":"ArcMap Raster Edit Suite (ARES) is an addin for ArcMap 10.x that enables value editing of single pixels on raster layer. This is an open-source software and the project is hosted at GitHub and published at SourceForge as well.","text":"ArcMap Raster Edit Suite (ARES) is an addin for ArcMap 10.x that enables value editing of single pixels on raster layer. This is an open-source software and the project is hosted at GitHub and published at SourceForge as well.This addin only works on ArcMap 10.0/10.1/10.2/10.3.A detail user guide could be found at: ArcMap Raster Edit SuiteDownloadARES 0.2.1The listed packages are stable releases and may not include up-to-date features. To download latest version, please visit the project page.InstallationSimply double-click the RasterEditor.esriAddIn in the package and ArcGIS AddIn installation wizzard will guide you. for more detail, check the wiki page Install and Uninstall.","categories":[{"name":"Project","slug":"Project","permalink":"http://haoliangyu.github.io/blog/categories/Project/"}],"tags":[{"name":"gis","slug":"gis","permalink":"http://haoliangyu.github.io/blog/tags/gis/"},{"name":"project","slug":"project","permalink":"http://haoliangyu.github.io/blog/tags/project/"},{"name":"arcgis","slug":"arcgis","permalink":"http://haoliangyu.github.io/blog/tags/arcgis/"}]}]}